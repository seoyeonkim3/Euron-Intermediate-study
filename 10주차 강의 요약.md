# 딥러닝 2단계: 심층 신경망 성능 향상시키기
## 4. 최적화 알고리즘
### 4-1. 미니 배치 경사하강법(mini-batch gradient descent)
- 배치 경사 하강법: 전체 훈련 샘플에 대해 훈련 후 경사 하강 진행 → 큰 데이터 세트 훈련하는데 많은 시간이 들어 경사 하강을 진행하기까지 오랜 시간이 소요
- 미니배치 경사 하강법: 전체 훈련 샘플을 작은 훈련 세트인 미니배치로 나눈 후, 미니배치 훈련 후 경사 하강 진행
  
![표기법](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/%EB%AF%B8%EB%8B%88%20%EB%B0%B0%EC%B9%98%20%ED%91%9C%EA%B8%B0%EB%B2%95.png?raw=true)

### 4-2. 미니 배치 경사하강법 이해하기(understanding mini-batch gradient descent)
![배치 경사 하강법](https://cphinf.pstatic.net/mooc/20180627_125/15300885625555rVb5_PNG/1.PNG?type=w760)
![미니배치 경사 하강법](https://cphinf.pstatic.net/mooc/20180627_148/1530088578609dbWSW_PNG/2.PNG?type=w760)
- 배치 경사 하강법: 매 반복마다 비용함수 J 값은 계속 감소
- 미니배치 경사 하강법: 전체적으로 비용 함수가 감소하는 경향, 많은 노이즈가 발생

- 미니배치 크기(mini-batch size): 가장 효율적이면서 비용함수 J를 줄이는 값을 찾아야 하는 하이퍼파라미터
  
  최적의 미니배치 크기: 1과 m(훈련 세트의 크기)의 중간값

### 4-3. 지수 가중 이동 평균(exponentially weighted average)
- 경사 하강법 및 미니배치 경사 하강법보다 더 효율적인 알고리즘을 이해하기 위해, 먼저 지수 가중 이동 평균 이해 필요
- 가중 이동 평균: 최근의 데이터에 더 많은 영향을 받는 데이터들의 평균 흐름을 계산, 최근 데이터 지점에 더 높은 가중치 부여
- β: 최적이 값을 찾아야 하는 하이퍼파라미터, 보통 0.9 사용
  
![식](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-3.%20%EC%A7%80%EC%88%98%20%EA%B0%80%EC%A4%91%20%EC%9D%B4%EB%8F%99%20%ED%8F%89%EA%B7%A0%20%EC%8B%9D.png?raw=true)

![vt의미](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-3.%20vt%EC%9D%98%EB%AF%B8.png?raw=true)

### 4-4. 지수 가중 이동 평균 이해하기(understanding exponentially weighted average)
